"""
LangChain åœ–è¡¨è™•ç†æ–¹æ³•æ¸¬è©¦

æ¸¬è©¦ LangChain å…§å»ºå·¥å…·è™•ç†ç›¸åŒPDFçš„æ•ˆæœï¼Œèˆ‡è‡ªè£½æ¨¡çµ„å°æ¯”
"""

import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# è¨­å®š UTF-8 ç·¨ç¢¼
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

# LangChain imports
try:
    from langchain_community.document_loaders import (
        UnstructuredPDFLoader,
        PyMuPDFLoader
    )
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import ChatOpenAI
    from langchain.schema import Document
    
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ LangChain å¥—ä»¶æœªå®Œå…¨å®‰è£: {e}")
    LANGCHAIN_AVAILABLE = False


class LangChainImageProcessor:
    """ä½¿ç”¨ LangChain å…§å»ºå·¥å…·çš„åœ–è¡¨è™•ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def process_with_unstructured(self, pdf_path: str) -> List[Document]:
        """ä½¿ç”¨ UnstructuredPDFLoader è™•ç†PDF"""
        
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain å¥—ä»¶ä¸å¯ç”¨")
        
        try:
            # ä½¿ç”¨ elements æ¨¡å¼å¯ä»¥è­˜åˆ¥ä¸åŒé¡å‹çš„å…§å®¹
            loader = UnstructuredPDFLoader(pdf_path, mode="elements")
            documents = loader.load()
            
            # åˆ†é¡ä¸åŒé¡å‹çš„å…ƒç´ 
            images = []
            tables = []
            texts = []
            
            for doc in documents:
                category = doc.metadata.get('category', 'unknown')
                
                if category == 'Image':
                    images.append(doc)
                elif category == 'Table':
                    tables.append(doc)
                else:
                    texts.append(doc)
            
            self.logger.info(f"Unstructuredè™•ç†çµæœ: {len(images)}åœ–åƒ, {len(tables)}è¡¨æ ¼, {len(texts)}æ–‡å­—")
            
            return {
                'images': images,
                'tables': tables,  
                'texts': texts,
                'all': documents
            }
            
        except Exception as e:
            self.logger.error(f"UnstructuredPDFLoader è™•ç†å¤±æ•—: {e}")
            return {'images': [], 'tables': [], 'texts': [], 'all': []}
    
    def process_with_pymupdf(self, pdf_path: str) -> List[Document]:
        """ä½¿ç”¨ PyMuPDFLoader è™•ç†PDF (å°æ¯”ç”¨)"""
        
        try:
            loader = PyMuPDFLoader(pdf_path)
            documents = loader.load()
            
            self.logger.info(f"PyMuPDFè™•ç†çµæœ: {len(documents)} å€‹æ–‡æª”")
            
            return documents
            
        except Exception as e:
            self.logger.error(f"PyMuPDFLoader è™•ç†å¤±æ•—: {e}")
            return []
    
    def analyze_elements(self, elements_result: Dict[str, List[Document]]) -> Dict[str, Any]:
        """åˆ†æå…ƒç´ è­˜åˆ¥çµæœ"""
        
        analysis = {
            'summary': {},
            'image_analysis': [],
            'table_analysis': [],
            'potential_captions': []
        }
        
        # çµ±è¨ˆæ¦‚è¦
        for key, docs in elements_result.items():
            if key != 'all':
                analysis['summary'][key] = len(docs)
        
        # åˆ†æåœ–åƒå…ƒç´ 
        for img_doc in elements_result.get('images', []):
            analysis['image_analysis'].append({
                'content': img_doc.page_content[:100] + "..." if len(img_doc.page_content) > 100 else img_doc.page_content,
                'metadata': img_doc.metadata,
                'page': img_doc.metadata.get('page_number', 'unknown')
            })
        
        # åˆ†æè¡¨æ ¼å…ƒç´ 
        for table_doc in elements_result.get('tables', []):
            analysis['table_analysis'].append({
                'content': table_doc.page_content[:100] + "..." if len(table_doc.page_content) > 100 else table_doc.page_content,
                'metadata': table_doc.metadata,
                'page': table_doc.metadata.get('page_number', 'unknown')
            })
        
        # åœ¨æ–‡å­—ä¸­å°‹æ‰¾å¯èƒ½çš„Caption
        import re
        caption_patterns = [
            r'åœ–\s*\d+[\.-]?\d*[ï¼š:].*',
            r'è¡¨\s*\d+[\.-]?\d*[ï¼š:].*',
            r'Figure\s*\d+[\.-]?\d*[ï¼š:.].*',
            r'Table\s*\d+[\.-]?\d*[ï¼š:.].*'
        ]
        
        for text_doc in elements_result.get('texts', []):
            content = text_doc.page_content
            for pattern in caption_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    analysis['potential_captions'].append({
                        'text': match,
                        'page': text_doc.metadata.get('page_number', 'unknown'),
                        'source': 'text_element'
                    })
        
        return analysis


def compare_approaches():
    """å°æ¯”LangChainæ–¹æ³•èˆ‡è‡ªè£½æ–¹æ³•"""
    
    print("ğŸ”„ é–‹å§‹å°æ¯” LangChain æ–¹æ³•èˆ‡è‡ªè£½æ–¹æ³•...")
    
    # æ¸¬è©¦æª”æ¡ˆ
    test_pdf = Path("ignore_file/test_pdf_data/sys_check_digital/è¨ˆæ¦‚ç¬¬ä¸€ç« .pdf")
    
    if not test_pdf.exists():
        print("âŒ æ¸¬è©¦PDFä¸å­˜åœ¨")
        return
    
    # 1. LangChainæ–¹æ³•æ¸¬è©¦
    print("\nğŸ“Š æ¸¬è©¦ LangChain UnstructuredPDFLoader...")
    
    if not LANGCHAIN_AVAILABLE:
        print("âŒ LangChain ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return
    
    try:
        processor = LangChainImageProcessor()
        
        # UnstructuredPDFLoader æ¸¬è©¦
        unstructured_result = processor.process_with_unstructured(str(test_pdf))
        analysis = processor.analyze_elements(unstructured_result)
        
        print(f"âœ… Unstructured è™•ç†å®Œæˆ:")
        print(f"   åœ–åƒå…ƒç´ : {analysis['summary'].get('images', 0)} å€‹")
        print(f"   è¡¨æ ¼å…ƒç´ : {analysis['summary'].get('tables', 0)} å€‹") 
        print(f"   æ–‡å­—å…ƒç´ : {analysis['summary'].get('texts', 0)} å€‹")
        print(f"   æ½›åœ¨Caption: {len(analysis['potential_captions'])} å€‹")
        
        # é¡¯ç¤ºç™¼ç¾çš„Caption
        if analysis['potential_captions']:
            print(f"\nğŸ” ç™¼ç¾çš„æ½›åœ¨Caption:")
            for i, caption in enumerate(analysis['potential_captions'][:5], 1):
                print(f"   {i}. {caption['text']} (é  {caption['page']})")
        
        # é¡¯ç¤ºåœ–åƒå…ƒç´ 
        if analysis['image_analysis']:
            print(f"\nğŸ–¼ï¸ åœ–åƒå…ƒç´ åˆ†æ:")
            for i, img in enumerate(analysis['image_analysis'][:3], 1):
                print(f"   {i}. é {img['page']}: {img['content']}")
        
        # é¡¯ç¤ºè¡¨æ ¼å…ƒç´   
        if analysis['table_analysis']:
            print(f"\nğŸ“‹ è¡¨æ ¼å…ƒç´ åˆ†æ:")
            for i, table in enumerate(analysis['table_analysis'][:3], 1):
                print(f"   {i}. é {table['page']}: {table['content']}")
        
    except Exception as e:
        print(f"âŒ LangChain æ¸¬è©¦å¤±æ•—: {e}")
        return
    
    # 2. å°æ¯”è‡ªè£½æ–¹æ³• (å¦‚æœå¯ç”¨)
    print(f"\nğŸ”„ å°æ¯”è‡ªè£½æ–¹æ³•çµæœ...")
    
    try:
        # èª¿ç”¨ç¾æœ‰çš„è‡ªè£½æ–¹æ³•
        from caption_extractor import PDFCaptionContextProcessor
        
        custom_processor = PDFCaptionContextProcessor()
        custom_results = custom_processor.process_pdf(str(test_pdf))
        
        print(f"âœ… è‡ªè£½æ–¹æ³•è™•ç†å®Œæˆ:")
        print(f"   è­˜åˆ¥Caption: {len(custom_results)} å€‹")
        
        # çµ±è¨ˆåˆ†æ
        custom_stats = custom_processor.get_processing_stats(custom_results)
        print(f"   ä¿¡å¿ƒåº¦ç¯„åœ: {custom_stats['confidence_stats']['min']:.3f} - {custom_stats['confidence_stats']['max']:.3f}")
        print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {custom_stats['confidence_stats']['avg']:.3f}")
        
    except ImportError:
        print("âš ï¸ è‡ªè£½æ–¹æ³•æ¨¡çµ„ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ è‡ªè£½æ–¹æ³•æ¸¬è©¦å¤±æ•—: {e}")
    
    # 3. çµè«–å’Œå»ºè­°
    print(f"\nğŸ¯ å°æ¯”åˆ†æçµè«–:")
    print(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # é€™è£¡å¯ä»¥æ ¹æ“šå¯¦éš›çµæœçµ¦å‡ºå»ºè­°
    print("âœ… LangChain å„ªå‹¢:")
    print("   - è‡ªå‹•å…ƒç´ åˆ†é¡ (åœ–åƒ/è¡¨æ ¼/æ–‡å­—)")
    print("   - æˆç†Ÿçš„PDFè§£æèƒ½åŠ›")
    print("   - èˆ‡å¤šæ¨¡æ…‹RAGæ•´åˆ")
    
    print("âœ… è‡ªè£½æ–¹æ³•å„ªå‹¢:")  
    print("   - é‡å°æ€§Captionè­˜åˆ¥")
    print("   - ä¸Šä¸‹æ–‡é…å°èƒ½åŠ›")
    print("   - å¯å®¢è£½åŒ–ä¿¡å¿ƒåº¦è©•ä¼°")
    
    print("\nğŸ’¡ å»ºè­°ç­–ç•¥:")
    print("   å¯è€ƒæ…®æ··åˆä½¿ç”¨: LangChainè² è²¬å…ƒç´ è­˜åˆ¥ï¼Œè‡ªè£½æ–¹æ³•è² è²¬ç²¾æº–é…å°")


def test_multimodal_integration():
    """æ¸¬è©¦å¤šæ¨¡æ…‹æ•´åˆå¯èƒ½æ€§"""
    
    print(f"\nğŸ”¬ æ¸¬è©¦å¤šæ¨¡æ…‹æ•´åˆå¯èƒ½æ€§...")
    
    # é€™è£¡å¯ä»¥æ¸¬è©¦LangChainçš„å¤šæ¨¡æ…‹RAGåŠŸèƒ½
    # ç”±æ–¼éœ€è¦é¡å¤–è¨­ç½®ï¼Œå…ˆæä¾›æ¶æ§‹æ¦‚å¿µ
    
    print("ğŸ’­ å¤šæ¨¡æ…‹æ•´åˆæ¦‚å¿µ:")
    print("   1. ä½¿ç”¨UnstructuredPDFLoaderè­˜åˆ¥å…ƒç´ ")
    print("   2. å°åœ–åƒå…ƒç´ ä½¿ç”¨Vision APIç”Ÿæˆæè¿°")  
    print("   3. å°‡æè¿°èˆ‡åŸæ–‡ä¸€èµ·å‘é‡åŒ–")
    print("   4. æª¢ç´¢æ™‚æ”¯æ´åœ–æ–‡æ··åˆçµæœ")
    
    print("\nâš ï¸ æ³¨æ„äº‹é …:")
    print("   - éœ€è¦Vision API (GPT-4V)")
    print("   - æˆæœ¬è€ƒé‡ (APIèª¿ç”¨è²»ç”¨)")
    print("   - è™•ç†é€Ÿåº¦ (ç¶²è·¯APIå»¶é²)")


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸš€ LangChain åœ–è¡¨è™•ç†æ–¹æ³•æ¸¬è©¦")
    print("=" * 50)
    
    # åŸ·è¡Œå°æ¯”æ¸¬è©¦
    compare_approaches()
    
    # æ¸¬è©¦å¤šæ¨¡æ…‹æ•´åˆ
    test_multimodal_integration()
    
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
    print("æ ¹æ“šæ¸¬è©¦çµæœæ±ºå®šæœ€é©åˆçš„æŠ€è¡“è·¯ç·šã€‚")