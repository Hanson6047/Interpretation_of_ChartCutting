#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éšæ®µA+Bæ•´åˆæ¸¬è©¦ï¼šCaptionè­˜åˆ¥ + LLMæè¿°ç”Ÿæˆ
"""

import sys
import os
import json
from datetime import datetime

# å°å…¥éšæ®µAå’ŒBçš„æ¨¡çµ„
from caption_extractor import PDFCaptionContextProcessor
from llm_description_generator import LLMDescriptionGenerator, DescriptionRequest

def test_stage_ab_integration():
    """æ¸¬è©¦éšæ®µA+Bæ•´åˆåŠŸèƒ½"""
    
    print("ğŸ”„ é–‹å§‹éšæ®µA+Bæ•´åˆæ¸¬è©¦")
    print("=" * 60)
    
    # æ­¥é©Ÿ1ï¼šä½¿ç”¨éšæ®µAæå–Caption
    print("ğŸ“‹ æ­¥é©Ÿ1ï¼šåŸ·è¡Œéšæ®µA - Captionè­˜åˆ¥")
    processor = PDFCaptionContextProcessor()
    pdf_file = "../../pdfFiles/è¨ˆæ¦‚ç¬¬ä¸€ç« .pdf"
    
    if not os.path.exists(pdf_file):
        print(f"âŒ æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨: {pdf_file}")
        return
    
    try:
        # åŸ·è¡ŒCaptionæå–
        stage_a_result = processor.process_pdf(pdf_file)
        print(f"âœ… éšæ®µAå®Œæˆï¼Œæ‰¾åˆ° {len(stage_a_result.caption_pairs)} å€‹Caption")
        
        # æ­¥é©Ÿ2ï¼šæº–å‚™éšæ®µBçš„è¼¸å…¥
        print("\nğŸ“‹ æ­¥é©Ÿ2ï¼šæº–å‚™LLMæè¿°ç”Ÿæˆè«‹æ±‚")
        description_requests = []
        
        # é¸æ“‡å‰5å€‹é«˜ä¿¡å¿ƒåº¦çš„Captioné€²è¡Œæ¸¬è©¦
        high_confidence_pairs = [
            pair for pair in stage_a_result.caption_pairs 
            if pair.caption_info.confidence > 0.5
        ][:5]
        
        for pair in high_confidence_pairs:
            caption = pair.caption_info
            context = pair.context_info
            
            # æº–å‚™ç›¸é—œå…§æ–‡
            related_context = []
            if context and context.related_paragraphs:
                related_context = [para[:100] for para in context.related_paragraphs[:3]]
            
            # å»ºç«‹æè¿°è«‹æ±‚
            request = DescriptionRequest(
                caption_text=caption.title,
                caption_type=caption.caption_type,
                caption_number=caption.number,
                related_context=related_context,
                page_number=caption.page
            )
            description_requests.append(request)
        
        print(f"âœ… æº–å‚™äº† {len(description_requests)} å€‹æè¿°ç”Ÿæˆè«‹æ±‚")
        
        # æ­¥é©Ÿ3ï¼šä½¿ç”¨éšæ®µBç”Ÿæˆæè¿°
        print("\nğŸ“‹ æ­¥é©Ÿ3ï¼šåŸ·è¡Œéšæ®µB - LLMæè¿°ç”Ÿæˆ")
        generator = LLMDescriptionGenerator()
        
        # ç”Ÿæˆæè¿°
        description_results = []
        for i, request in enumerate(description_requests, 1):
            print(f"\nğŸ”„ æ­£åœ¨è™•ç† {i}/{len(description_requests)}: {request.caption_type} {request.caption_number}")
            
            result = generator.generate_description(request)
            description_results.append(result)
            
            if result.success:
                print(f"âœ… ç”ŸæˆæˆåŠŸ (ä¿¡å¿ƒåº¦: {result.confidence_score:.2f})")
                print(f"ğŸ“ æè¿°: {result.generated_description[:100]}...")
            else:
                print(f"âŒ ç”Ÿæˆå¤±æ•—: {result.error_message}")
        
        # æ­¥é©Ÿ4ï¼šæ•´åˆçµæœåˆ†æ
        print("\nğŸ“‹ æ­¥é©Ÿ4ï¼šæ•´åˆçµæœåˆ†æ")
        analyze_integration_results(stage_a_result, description_results)
        
        # æ­¥é©Ÿ5ï¼šä¿å­˜æ•´åˆçµæœ
        print("\nğŸ“‹ æ­¥é©Ÿ5ï¼šä¿å­˜æ•´åˆçµæœ")
        save_integration_results(stage_a_result, description_results)
        
        print("\nâœ… éšæ®µA+Bæ•´åˆæ¸¬è©¦å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()

def analyze_integration_results(stage_a_result, description_results):
    """åˆ†ææ•´åˆçµæœ"""
    
    print("\nğŸ“Š æ•´åˆçµæœåˆ†æ")
    print("-" * 40)
    
    # éšæ®µAçµ±è¨ˆ
    total_captions = len(stage_a_result.caption_pairs)
    high_conf_captions = len([p for p in stage_a_result.caption_pairs if p.caption_info.confidence > 0.5])
    
    print(f"éšæ®µAçµ±è¨ˆ:")
    print(f"  â€¢ ç¸½Captionæ•¸: {total_captions}")
    print(f"  â€¢ é«˜ä¿¡å¿ƒåº¦(>0.5): {high_conf_captions}")
    print(f"  â€¢ æˆåŠŸç‡: {high_conf_captions/total_captions*100:.1f}%")
    
    # éšæ®µBçµ±è¨ˆ
    successful_descriptions = [r for r in description_results if r.success]
    total_tokens = sum(r.token_usage.get('total_tokens', 0) for r in successful_descriptions)
    avg_confidence = sum(r.confidence_score for r in successful_descriptions) / len(successful_descriptions) if successful_descriptions else 0
    
    print(f"\néšæ®µBçµ±è¨ˆ:")
    print(f"  â€¢ è™•ç†è«‹æ±‚æ•¸: {len(description_results)}")
    print(f"  â€¢ æˆåŠŸç”Ÿæˆæ•¸: {len(successful_descriptions)}")
    print(f"  â€¢ æˆåŠŸç‡: {len(successful_descriptions)/len(description_results)*100:.1f}%")
    print(f"  â€¢ å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.2f}")
    print(f"  â€¢ ç¸½Tokenä½¿ç”¨: {total_tokens}")
    
    # å“è³ªè©•ä¼°
    print(f"\nğŸ“ˆ å“è³ªè©•ä¼°:")
    for i, result in enumerate(successful_descriptions):
        print(f"  {i+1}. ä¿¡å¿ƒåº¦: {result.confidence_score:.2f}, æè¿°é•·åº¦: {len(result.generated_description)}")

def save_integration_results(stage_a_result, description_results):
    """ä¿å­˜æ•´åˆçµæœ"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"stage_ab_integration_results_{timestamp}.json"
    
    # æº–å‚™ä¿å­˜æ•¸æ“š
    integration_data = {
        "test_time": timestamp,
        "stage_a_summary": {
            "total_captions": len(stage_a_result.caption_pairs),
            "caption_types": {},
            "average_confidence": 0
        },
        "stage_b_summary": {
            "total_requests": len(description_results),
            "successful_generations": len([r for r in description_results if r.success]),
            "total_tokens_used": sum(r.token_usage.get('total_tokens', 0) for r in description_results),
            "average_confidence": 0
        },
        "integrated_results": []
    }
    
    # è¨ˆç®—éšæ®µAçµ±è¨ˆ
    caption_types = {}
    total_conf_a = 0
    for pair in stage_a_result.caption_pairs:
        caption_type = pair.caption_info.caption_type
        caption_types[caption_type] = caption_types.get(caption_type, 0) + 1
        total_conf_a += pair.caption_info.confidence
    
    integration_data["stage_a_summary"]["caption_types"] = caption_types
    integration_data["stage_a_summary"]["average_confidence"] = total_conf_a / len(stage_a_result.caption_pairs)
    
    # è¨ˆç®—éšæ®µBçµ±è¨ˆ
    successful_results = [r for r in description_results if r.success]
    if successful_results:
        avg_conf_b = sum(r.confidence_score for r in successful_results) / len(successful_results)
        integration_data["stage_b_summary"]["average_confidence"] = avg_conf_b
    
    # æ•´åˆçµæœè©³æƒ…
    for i, result in enumerate(description_results):
        integrated_item = {
            "index": i + 1,
            "original_caption": result.original_caption,
            "generated_description": result.generated_description,
            "stage_b_confidence": result.confidence_score,
            "success": result.success,
            "processing_time": result.processing_time,
            "token_usage": result.token_usage
        }
        integration_data["integrated_results"].append(integrated_item)
    
    # ä¿å­˜æª”æ¡ˆ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(integration_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•´åˆçµæœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    try:
        test_stage_ab_integration()
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦è¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()